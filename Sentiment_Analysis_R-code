install.pacakges("RtextTools")
install.packages("e1071")
install.packages("dplyr")
install.packages("caret")
install.packages("doMC")
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
library(tm)
library(RTextTools)
library(e1071)
library(dplyr)
library(caret)
library(doMC)
registerDoMC(cores=detectCores()) 
library("SnowballC")
library("wordcloud")
library("RColorBrewer")

#set directory
setwd("E:/Data mining_files")
getwd()

#read data 
library(readxl)
project_xlsb <- read_excel("project.xlsb.xlsx", 
                             +     sheet = "Sheet1")
View(project_xlsb)

# separate only columns needed for analysis
df <- project_xlsb[ , -c(2,3,4,5)]
glimpse(df)

#randomise data set
set.seed(1)
df <- df[sample(nrow(df)), ]
df <- df[sample(nrow(df)), ]
glimpse(df)

# analyse the data
str(df)

#factorise the type
df$TYPE<- as.factor(df$TYPE)
str(df$TYPE)
table(df$TYPE)

#bag of words tokenization
corpus <- Corpus(VectorSource(df$TWEET))
corpus
inspect(corpus[1:3])#no of characters in message
print(corpus)
as.character(corpus[[1]]) #displays the message

#data clean up
#convert to lower case
corpus <- tm_map(corpus,    content_transformer(tolower))
as.character(corpus[[1]])

# remove numbers
corpus <- tm_map(corpus, removeNumbers)
as.character(corpus[[1]])

# remove stop words
corpus <- tm_map(corpus, removeWords, stopwords())
as.character(corpus[[1]])

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
as.character(corpus[[1]])

# words to root form
corpus <- tm_map(corpus, stemDocument)
as.character(corpus[[1]])

#replace special characters with white space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace, "\\|")

# remove white space
corpus.clean <- tm_map(corpus, stripWhitespace)
as.character(corpus.clean[[1]])

#dtm
dtm <- DocumentTermMatrix(corpus.clean)
inspect(dtm[40:50, 10:15])

# word cloud
wordcloud(corpus.clean, min.freq = 10, max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
positive <- subset(corpus.clean, df$TYPE == 4 )
negative <- subset(corpus.clean, df$TYPE == 0)
neutral <- subset(corpus.clean, df$TYPE==1)
wordcloud(positive, max.words = 30, scale = c(3, 0.1))
wordcloud(negative, max.words = 40, scale = c(3, 0.1))
wordcloud(neutral, max.words = 40, scale = c(3, 0.1))

# frequent terms in DTM
findFreqTerms(dtm, lowfreq = 4)

#plot word frequencies
barplot(

#partitioning data
df.train <- df[1:4800,]
df.test <- df[4801:5999,]

dtm.train <- dtm[1:4800,]
dtm.test <- dtm[4801:5999,]

corpus.clean.train <- corpus.clean[1:4800]
corpus.clean.test <- corpus.clean[4801:5999]

#Feature selection
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train, 5)
length((fivefreq))
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fivefreq))
dim(dtm.train.nb)
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))
dim(dtm.test.nb)

# Naive Bayes alogorithm:
# Function to convert the word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

# Apply the convert_count function to get final training and testing DTMs
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)

# Training naive bayers algorithm:
# Train the classifier
system.time( classifier <- naiveBayes(trainNB, df.train$TYPE, laplace = 1) )

# Test the predictions:
# Use the NB classifier we built to make predictions on the test set.
system.time( pred <- predict(classifier, newdata=testNB) )

# Create a truth table by tabulating the predicted class labels with the actual class labels 
table("Predictions"= pred,  "Actual" = df.test$TYPE )

# Confusion matrix
conf.mat <- confusionMatrix(pred, df.test$TYPE)
conf.mat

